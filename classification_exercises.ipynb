{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8245b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "import os\n",
    "import seaborn as sns\n",
    "import acquire\n",
    "import prepare\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fd532",
   "metadata": {},
   "source": [
    "# In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1e6cd",
   "metadata": {},
   "source": [
    "# Create a pandas dataframe, df_iris, from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58bef7",
   "metadata": {},
   "source": [
    "print the first 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bd86e",
   "metadata": {},
   "source": [
    "print the number of rows and columns (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82b1c2",
   "metadata": {},
   "source": [
    "print the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae463a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346461c",
   "metadata": {},
   "source": [
    "print the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5086e",
   "metadata": {},
   "source": [
    "print the summary statistics for each of the numeric variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ce25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3c5be",
   "metadata": {},
   "source": [
    "Would you recommend rescaling the data based on these statistics?\n",
    "\n",
    "It does not appear rescaling is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8f312",
   "metadata": {},
   "source": [
    "# Read the Table1_CustDetails table from the Excel_Exercises.xlsx file into a dataframe named df_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94042d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel = pd.read_excel('Excel_Exercises.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b9b9c",
   "metadata": {},
   "source": [
    "assign the first 100 rows to a new dataframe, df_excel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77302d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample = df_excel.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92feb38",
   "metadata": {},
   "source": [
    "print the number of rows of your original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df_excel has',len(df_excel),'rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38193d",
   "metadata": {},
   "source": [
    "print the first 5 column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce57140",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_excel.columns[0:5].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3f708",
   "metadata": {},
   "source": [
    "print the column names that have a data type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.select_dtypes(include='object').columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b01d7",
   "metadata": {},
   "source": [
    "compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299fe0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series with only the columns containing numbers, and their minimum/maximum values\n",
    "mins = df_excel.select_dtypes(include='float64').min()\n",
    "maxes = df_excel.select_dtypes(include='float64').max()\n",
    "\n",
    "ranges = pd.concat([mins, maxes], axis=1)\n",
    "ranges.columns =['min_value', 'max_value']\n",
    "ranges.insert(2, \"Range\", ranges.max_value - ranges.min_value , True)\n",
    "ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da518e86",
   "metadata": {},
   "source": [
    "# Read the data from this google sheet into a dataframe, df_google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ff7eb",
   "metadata": {},
   "source": [
    "print the first three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'\n",
    "csv_import_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "df_google = pd.read_csv(csv_import_url)\n",
    "df_google.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c575d37",
   "metadata": {},
   "source": [
    "Print the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfde48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c987d17",
   "metadata": {},
   "source": [
    "Print the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5aa350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9e33d",
   "metadata": {},
   "source": [
    "Print the datatype of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed008fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823afb1a",
   "metadata": {},
   "source": [
    "print the summary statistics for each of the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.select_dtypes(include=['int64', 'float64']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6c247",
   "metadata": {},
   "source": [
    "Print the unique values for each of your categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a76794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf815f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values from 'Survived':\",df_google['Survived'].unique())\n",
    "print(\"Unique values from 'Pclass':\",df_google['Pclass'].unique())\n",
    "print(\"Unique values from 'Sex':\",df_google['Sex'].unique())\n",
    "print(\"Unique values from 'Embarked':\",df_google['Embarked'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data acquisition functions from acquire.py file I created\n",
    "acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "acquire.get_telco_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085b057",
   "metadata": {},
   "source": [
    "The end product of this exercise should be the specified functions in a python script named prepare.py. Do these in your classification_exercises.ipynb first, then transfer to the prepare.py file.\n",
    "\n",
    "This work should all be saved in your local classification-exercises repo. Then add, commit, and push your changes.\n",
    "\n",
    "Using the Iris Data:\n",
    "\n",
    "Use the function defined in acquire.py to load the iris data.\n",
    "\n",
    "Drop the species_id and measurement_id columns.\n",
    "\n",
    "Rename the species_name column to just species.\n",
    "\n",
    "Create dummy variables of the species name and concatenate onto the iris dataframe. (This is for practice, we don't always have to encode the target, but if we used species as a feature, we would need to encode it).\n",
    "\n",
    "Create a function named prep_iris that accepts the untransformed iris data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris data using our function in the acquire file\n",
    "iris_df = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c866a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that it loaded and show the first few lines of data\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the species_id and measurement_id columns\n",
    "iris_df = iris_df.drop(columns=['species_id','measurement_id'])\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the species_name column to species\n",
    "\n",
    "iris_df.rename(columns = {'species_name':'species'}, inplace = True)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb64cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables of the species name    \n",
    "dummy_df = pd.get_dummies(iris_df['species'], dummy_na=False, drop_first=False)\n",
    "\n",
    "# concatenate onto the iris dataframe.\n",
    "iris_df = pd.concat([iris_df, dummy_df], axis=1)\n",
    "iris_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab670707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function named prep_iris that accepts the untransformed iris data, \n",
    "# and returns the data with the transformations above applied.\n",
    "\n",
    "def prep_iris(df):\n",
    "    # Drop the species_id and measurement_id columns\n",
    "    df = df.drop(columns=['species_id','measurement_id'])\n",
    "    \n",
    "    # rename the species_name column to species\n",
    "    df.rename(columns = {'species_name':'species'}, inplace = True)\n",
    "    \n",
    "    # Create dummy variables of the species name    \n",
    "    dummy_df = pd.get_dummies(df['species'], dummy_na=False, drop_first=False)\n",
    "\n",
    "    # concatenate onto the iris dataframe.\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    \n",
    "    # return the converted iris dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08aeb7",
   "metadata": {},
   "source": [
    "Using the Titanic dataset\n",
    "\n",
    "Use the function defined in acquire.py to load the Titanic data.\n",
    "\n",
    "Drop any unnecessary, unhelpful, or duplicated columns.\n",
    "\n",
    "Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "Create a function named prep_titanic that accepts the raw titanic data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d40e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6bb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns (pclass)\n",
    "df = df.drop(columns=['pclass'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables of the categorical columns   \n",
    "dummy_df = pd.get_dummies(df[['sex','class','embark_town']], dummy_na=False, drop_first=False)\n",
    "\n",
    "# concatenate onto the titanic dataframe.\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2016435",
   "metadata": {},
   "source": [
    "Use the function defined in acquire.py to load the Telco data.\n",
    "\n",
    "Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns but keeping the corresponding string values, for example.\n",
    "\n",
    "Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "Create a function named prep_telco that accepts the raw telco data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d524798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_telco_data()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any unnecessary, unhelpful, or duplicated columns. \n",
    "df = df.drop(columns=['contract_type_id','internet_service_type_id', 'payment_type_id', 'contract_type_id.1',\n",
    "                      'payment_type_id.1', 'monthly_charges.1','total_charges.1','paperless_billing.1',])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical columns. \n",
    "# Create dummy variables of the categorical columns and concatenate them onto the dataframe.\n",
    "\n",
    "# Create dummy variables of the categorical columns  \n",
    "dummy_df = pd.get_dummies(df[['gender','contract_type','internet_service_type']], dummy_na=False, drop_first=False)\n",
    "\n",
    "# concatenate onto the telco dataframe.\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18762602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing functions in prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_df = acquire.get_iris_data()\n",
    "i_df = prepare.prep_iris(i_df)\n",
    "i_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_df.to_csv('iris.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af901858",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = acquire.get_titanic_data()\n",
    "t_df = prepare.prep_titanic(t_df)\n",
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df.to_csv('titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7faecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tel_df = acquire.get_telco_data()\n",
    "tel_df = prepare.prep_telco(tel_df)\n",
    "tel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created split functions to split telco, iris, and titanic into train, validate, and test data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fddb88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire iris data\n",
    "i_df = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c145ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call split function on iris dataframe\n",
    "target = 'species'\n",
    "train_iris, validate_iris, test_iris = prepare.my_split(i_df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5975f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 8), (30, 8), (30, 8))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate shapes of data_sets look correct\n",
    "train_iris.shape,test_iris.shape,validate_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6a3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire telco data\n",
    "tel_df = acquire.get_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f887d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call split function\n",
    "target = 'churn'\n",
    "train_telco, validate_telco, test_telco = prepare.my_split(tel_df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd452eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4225, 28), (1409, 28), (1409, 28))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate shapes of data_sets look correct\n",
    "train_telco.shape, validate_telco.shape, test_telco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402e29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get titanic data\n",
    "tit_df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "200dd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call split function\n",
    "target = 'survived'\n",
    "train_titanic, validate_titanic, test_titanic = prepare.my_split(tit_df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d84d8325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((534, 20), (178, 20), (179, 20))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate shapes of data_sets look correct\n",
    "train_titanic.shape, validate_titanic.shape, test_titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018d0d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d814c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890050b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
